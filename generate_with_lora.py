from omegaconf import OmegaConf, ListConfig, DictConfig
import os
import sys
import torch
from PIL import Image
import argparse
from pathlib import Path

from dual_process import dig_helpers


def init_config():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ…£ç¿’ã«å¾“ã£ãŸè¨­å®šåˆæœŸåŒ–"""
    global_config = OmegaConf.create({})
    for config_name in sys.argv[1].split(","):
        global_config = OmegaConf.merge(global_config, OmegaConf.load(config_name))
    if len(sys.argv) > 2:
        cli_overrides = OmegaConf.from_cli(sys.argv[2:])
        global_config = OmegaConf.merge(global_config, cli_overrides)
    OmegaConf.resolve(global_config)
    
    # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®è¨­å®š
    save_folder = global_config.get("save_folder", "generated_with_lora")
    if not os.path.exists(save_folder):
        os.makedirs(save_folder, exist_ok=True)
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    OmegaConf.save(global_config, f"{save_folder}/generation_config.yaml")
    return global_config, save_folder


def load_lora_weights(pipe, lora_files, lora_weights=None, lora_config=None):
    """
    è¤‡æ•°ã®LoRAãƒ•ã‚¡ã‚¤ãƒ«ã‚’é‡ã¿ä»˜ãã§åŠ ç®—ã—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é©ç”¨
    
    Args:
        pipe: ç”»åƒç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        lora_files: LoRAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        lora_weights: å„LoRAã®é‡ã¿ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã¦1.0ï¼‰
        lora_config: LoRAè¨­å®šè¾æ›¸
    """
    # ListConfig/DictConfigã‚’é€šå¸¸ã®Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    if isinstance(lora_files, (ListConfig, DictConfig)):
        lora_files = OmegaConf.to_container(lora_files, resolve=True)
    if isinstance(lora_weights, (ListConfig, DictConfig)):
        lora_weights = OmegaConf.to_container(lora_weights, resolve=True)
    
    # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
    if not isinstance(lora_files, list):
        lora_files = [lora_files]
    
    if lora_weights is None:
        lora_weights = [1.0] * len(lora_files)
    elif not isinstance(lora_weights, list):
        lora_weights = [lora_weights] * len(lora_files)
    
    # é‡ã¿ãƒªã‚¹ãƒˆã®é•·ã•ã‚’èª¿æ•´
    if len(lora_weights) < len(lora_files):
        lora_weights.extend([1.0] * (len(lora_files) - len(lora_weights)))
    elif len(lora_weights) > len(lora_files):
        lora_weights = lora_weights[:len(lora_files)]
    
    print(f"Loading and combining {len(lora_files)} LoRA files with weights: {lora_weights}")
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ
    default_lora_kwargs = {
        "r": 16,
        "lora_lr": 5e-5,
        "init_lora_weights": "gaussian",
        "lora_dropout": 0.0,
        "target_modules": [
            "attn.to_k", "attn.to_q", "attn.to_v", "attn.to_out.0",
            "attn.add_k_proj", "attn.add_q_proj"
        ]
    }
    
    if lora_config and "lora_kwargs" in lora_config:
        lora_kwargs = lora_config["lora_kwargs"]
        if isinstance(lora_kwargs, DictConfig):
            lora_kwargs = OmegaConf.to_container(lora_kwargs, resolve=True)
    else:
        lora_kwargs = default_lora_kwargs
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ
    lora_params = dig_helpers.create_lora(pipe, **lora_kwargs)
    
    # è¤‡æ•°LoRAãƒ•ã‚¡ã‚¤ãƒ«ã‚’é‡ã¿ä»˜ãã§åŠ ç®—
    if lora_files:
        combined_state_dict = {}
        
        for i, (lora_file, weight) in enumerate(zip(lora_files, lora_weights)):
            lora_file = str(lora_file)
            
            if not os.path.exists(lora_file):
                print(f"Warning: LoRA file not found: {lora_file}")
                continue
            
            print(f"Loading LoRA {i+1}/{len(lora_files)}: {lora_file} (weight: {weight})")
            
            try:
                # LoRAé‡ã¿ã‚’èª­ã¿è¾¼ã¿
                lora_state = torch.load(lora_file, map_location=pipe.device)
                
                # é‡ã¿ä»˜ãã§åŠ ç®—
                for param_name, param_tensor in lora_state.items():
                    if param_name not in combined_state_dict:
                        # æœ€åˆã®LoRAã®å ´åˆã¯é‡ã¿ä»˜ãã§ã‚³ãƒ”ãƒ¼
                        combined_state_dict[param_name] = weight * param_tensor.to(pipe.device)
                    else:
                        # 2ç•ªç›®ä»¥é™ã®LoRAã¯é‡ã¿ä»˜ãã§åŠ ç®—
                        combined_state_dict[param_name] += weight * param_tensor.to(pipe.device)
                
                print(f"  âœ… Added LoRA {i+1} with weight {weight}")
                
            except Exception as e:
                print(f"  âŒ Failed to load LoRA {lora_file}: {e}")
                continue
        
        # åŠ ç®—ã•ã‚ŒãŸé‡ã¿ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é©ç”¨
        if combined_state_dict:
            try:
                # dig_helpersã®æ–¹æ³•ã§é©ç”¨
                backbone = dig_helpers.get_backbone(pipe)
                
                # é‡ã¿ã‚­ãƒ¼ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
                formatted_state_dict = {}
                for name, param in combined_state_dict.items():
                    # .weight ã‚’ .default.weight ã«å¤‰æ›
                    formatted_name = name.replace(".weight", ".default.weight")
                    formatted_state_dict[formatted_name] = param
                
                # state_dictã‚’èª­ã¿è¾¼ã¿
                backbone.load_state_dict(formatted_state_dict, strict=False)
                
                print(f"âœ… Successfully combined and loaded {len(combined_state_dict)} LoRA parameters")
                print(f"ğŸ“Š Combined LoRA weights: {dict(zip(lora_files, lora_weights))}")
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«LoRAé©ç”¨ãƒ•ãƒ©ã‚°ã‚’è¨˜éŒ²
                pipe._lora_weight = 1.0  # åŠ ç®—æ¸ˆã¿ãªã®ã§ç”Ÿæˆæ™‚ã¯1.0ã§é©ç”¨
                pipe._combined_lora_info = {
                    'files': lora_files,
                    'weights': lora_weights,
                    'num_params': len(combined_state_dict)
                }
                
                # ãƒ‡ãƒãƒƒã‚°: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
                if hasattr(backbone, "peft_config"):
                    print(f"âœ… LoRA adapter created: {list(backbone.peft_config.keys())}")
                    print(f"âœ… Active adapters: {backbone.active_adapters()}")
                else:
                    print("âŒ No LoRA adapter found!")
                
            except Exception as e:
                print(f"âŒ Failed to apply combined LoRA weights: {e}")
                return pipe
        else:
            print("âŒ No valid LoRA weights to combine")
            return pipe
    
    return pipe


def generate_images(pipe, config, save_folder):
    """ç”»åƒç”Ÿæˆã‚’å®Ÿè¡Œï¼ˆLoRAé©ç”¨ç‰ˆã¨ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆã®æ¯”è¼ƒç”Ÿæˆå¯¾å¿œï¼‰"""
    generation_config = config.get("generation", {})
    
    # ListConfig/DictConfigã‚’é©åˆ‡ã«å‡¦ç†
    if isinstance(generation_config, DictConfig):
        generation_config = OmegaConf.to_container(generation_config, resolve=True)
    
    prompts = generation_config.get("prompts", ["Photo of a man"])
    if isinstance(prompts, (ListConfig, DictConfig)):
        prompts = OmegaConf.to_container(prompts, resolve=True)
    elif isinstance(prompts, str):
        prompts = [prompts]
    
    num_images_per_prompt = generation_config.get("num_images_per_prompt", 4)
    
    # generator_kwargsã‚’è¨­å®šã‹ã‚‰å–å¾—
    generator_kwargs = config.get("generator_kwargs", {
        "height": 1024,
        "width": 1024,
        "num_inference_steps": 4,
        "guidance_scale": 3.5
    })
    
    # DictConfigã‚’é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›
    if isinstance(generator_kwargs, DictConfig):
        generator_kwargs = OmegaConf.to_container(generator_kwargs, resolve=True)
    
    # æ¯”è¼ƒç”Ÿæˆã®è¨­å®šã‚’å–å¾—
    lora_config = config.get("lora_generation", {})
    if isinstance(lora_config, DictConfig):
        lora_config = OmegaConf.to_container(lora_config, resolve=True)
    
    generate_original = lora_config.get("generate_original", False)
    original_folder = lora_config.get("original_folder", "original")
    lora_folder = lora_config.get("lora_folder", "with_lora")
    
    print(f"Generating images with {len(prompts)} prompts...")
    
    # ãƒ‡ãƒãƒƒã‚°: LoRAçŠ¶æ…‹ç¢ºèª
    backbone = dig_helpers.get_backbone(pipe)
    has_lora = hasattr(backbone, "peft_config")
    if has_lora:
        print(f"ğŸ”§ LoRA adapters available: {list(backbone.peft_config.keys())}")
        lora_weight = getattr(pipe, '_lora_weight', 1.0)
        print(f"ğŸ”§ LoRA application weight: {lora_weight}")
        
        # è¤‡æ•°LoRAæƒ…å ±ã®è¡¨ç¤º
        combined_info = getattr(pipe, '_combined_lora_info', None)
        if combined_info:
            print(f"ğŸ“Š Combined LoRA info:")
            for i, (file, weight) in enumerate(zip(combined_info['files'], combined_info['weights'])):
                print(f"  {i+1}. {os.path.basename(file)}: weight={weight}")
            print(f"  Total parameters: {combined_info['num_params']}")
    else:
        print("ğŸ”§ No LoRA adapters - using base model")
    
    if generate_original:
        print(f"ğŸ“Š Comparison mode enabled:")
        print(f"  - Original images â†’ {original_folder}/")
        print(f"  - LoRA images â†’ {lora_folder}/")
    
    for prompt_idx, prompt in enumerate(prompts):
        print(f"\nPrompt {prompt_idx + 1}/{len(prompts)}: '{prompt}'")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
        prompt_folder = os.path.join(save_folder, f"prompt_{prompt_idx:03d}")
        os.makedirs(prompt_folder, exist_ok=True)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(os.path.join(prompt_folder, "prompt.txt"), "w") as f:
            f.write(prompt)
        
        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå›ºæœ‰æƒ…å ±ã‚’è¿½åŠ 
        main_config_path = os.path.join(save_folder, "generation_config.yaml")
        prompt_config_path = os.path.join(prompt_folder, "generation_config.yaml")
        if os.path.exists(main_config_path):
            # ãƒ¡ã‚¤ãƒ³è¨­å®šã‚’èª­ã¿è¾¼ã¿
            with open(main_config_path, 'r') as f:
                prompt_config = OmegaConf.load(f)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå›ºæœ‰æƒ…å ±ã‚’è¿½åŠ 
            from datetime import datetime
            prompt_config.current_prompt = {
                'index': prompt_idx,
                'text': prompt,
                'folder_name': f"prompt_{prompt_idx:03d}",
                'generation_timestamp': datetime.now().isoformat()
            }
            
            # è¤‡æ•°LoRAæƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
            combined_info = getattr(pipe, '_combined_lora_info', None)
            if combined_info:
                prompt_config.applied_lora_info = combined_info
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå›ºæœ‰ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            OmegaConf.save(prompt_config, prompt_config_path)
            print(f"  ğŸ“‹ Generated prompt-specific generation_config.yaml")
        else:
            # ãƒ¡ã‚¤ãƒ³è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªã‚³ãƒ”ãƒ¼
            import shutil
            shutil.copy2(main_config_path, prompt_config_path) if os.path.exists(main_config_path) else None
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒç”¨ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if generate_original:
            original_subfolder = os.path.join(prompt_folder, original_folder)
            os.makedirs(original_subfolder, exist_ok=True)
        
        # LoRAç”»åƒç”¨ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆLoRAãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if has_lora:
            lora_subfolder = os.path.join(prompt_folder, lora_folder)
            os.makedirs(lora_subfolder, exist_ok=True)
        
        for img_idx in range(num_images_per_prompt):
            # ã‚·ãƒ¼ãƒ‰è¨­å®š
            seed = generation_config.get("seed", 42) + img_idx
            
            # 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒç”Ÿæˆï¼ˆLoRAãªã—ï¼‰
            if generate_original:
                print(f"  Generating original image {img_idx + 1}/{num_images_per_prompt}...")
                try:
                    generator = torch.Generator(device=pipe.device).manual_seed(seed)
                    
                    with torch.no_grad():
                        # LoRAãªã—ã§ç”Ÿæˆï¼ˆweight=0ï¼‰
                        if has_lora:
                            with dig_helpers.LoraManager(pipe, enter_weights=0):
                                result = pipe(
                                    prompt=prompt,
                                    generator=generator,
                                    **generator_kwargs
                                )
                                original_image = result.images[0]
                        else:
                            result = pipe(
                                prompt=prompt,
                                generator=generator,
                                **generator_kwargs
                            )
                            original_image = result.images[0]
                    
                    # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒä¿å­˜
                    original_path = os.path.join(prompt_folder, original_folder, f"original_{img_idx:03d}_seed_{seed}.png")
                    original_image.save(original_path)
                    print(f"    Saved original: {os.path.basename(original_path)}")
                    
                except Exception as e:
                    print(f"    Error generating original image {img_idx}: {e}")
                    continue
            
            # 2. LoRAé©ç”¨ç”»åƒç”Ÿæˆ
            if has_lora:
                print(f"  Generating LoRA image {img_idx + 1}/{num_images_per_prompt}...")
                try:
                    generator = torch.Generator(device=pipe.device).manual_seed(seed)
                    
                    with torch.no_grad():
                        # LoRAManagerã‚’ä½¿ã£ã¦LoRAé‡ã¿ã‚’é©ç”¨
                        lora_weight = getattr(pipe, '_lora_weight', 1.0)
                        
                        with dig_helpers.LoraManager(pipe, enter_weights=lora_weight):
                            result = pipe(
                                prompt=prompt,
                                generator=generator,
                                **generator_kwargs
                            )
                            lora_image = result.images[0]
                    
                    # LoRAç”»åƒä¿å­˜
                    lora_path = os.path.join(prompt_folder, lora_folder, f"lora_{img_idx:03d}_seed_{seed}.png")
                    lora_image.save(lora_path)
                    print(f"    Saved LoRA: {os.path.basename(lora_path)}")
                    
                except Exception as e:
                    print(f"    Error generating LoRA image {img_idx}: {e}")
                    continue
            else:
                # LoRAãŒãªã„å ´åˆã¯é€šå¸¸ã®ç”»åƒç”Ÿæˆ
                print(f"  Generating image {img_idx + 1}/{num_images_per_prompt} (no LoRA)...")
                try:
                    generator = torch.Generator(device=pipe.device).manual_seed(seed)
                    
                    with torch.no_grad():
                        result = pipe(
                            prompt=prompt,
                            generator=generator,
                            **generator_kwargs
                        )
                        image = result.images[0]
                    
                    # é€šå¸¸ç”»åƒä¿å­˜
                    image_path = os.path.join(prompt_folder, f"image_{img_idx:03d}_seed_{seed}.png")
                    image.save(image_path)
                    print(f"    Saved: {os.path.basename(image_path)}")
                    
                except Exception as e:
                    print(f"    Error generating image {img_idx}: {e}")
                    continue
    
    print(f"\nGeneration completed! Results saved to: {save_folder}")
    if generate_original and has_lora:
        print(f"ğŸ“Š Comparison images generated:")
        print(f"  - Original images (LoRA weight=0): {original_folder}/")
        print(f"  - LoRA images (combined LoRA applied): {lora_folder}/")
        
        # è¤‡æ•°LoRAæƒ…å ±ã®å†è¡¨ç¤º
        combined_info = getattr(pipe, '_combined_lora_info', None)
        if combined_info:
            print(f"ğŸ”— Combined LoRA summary:")
            for i, (file, weight) in enumerate(zip(combined_info['files'], combined_info['weights'])):
                print(f"    {os.path.basename(file)}: {weight}")
    elif has_lora:
        combined_info = getattr(pipe, '_combined_lora_info', None)
        if combined_info:
            print(f"ğŸ”— Generated with combined LoRA:")
            for i, (file, weight) in enumerate(zip(combined_info['files'], combined_info['weights'])):
                print(f"    {os.path.basename(file)}: {weight}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage: python generate_with_lora.py <config_files> [overrides...]")
        print("Example: python generate_with_lora.py configs/base.yaml,configs/pipe/schnell.yaml,configs/generation/my_lora.yaml")
        print("         python generate_with_lora.py configs/base.yaml,configs/pipe/schnell.yaml save_folder=my_output")
        sys.exit(1)
    
    # è¨­å®šåˆæœŸåŒ–
    config, save_folder = init_config()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿
    print("Loading pipeline...")
    pipe = dig_helpers.load_pipe(**config["pipe_kwargs"])
    
    # LoRAãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    lora_config = config.get("lora_generation", {})
    if isinstance(lora_config, DictConfig):
        lora_config = OmegaConf.to_container(lora_config, resolve=True)
    
    lora_files = lora_config.get("lora_files", [])
    lora_weights = lora_config.get("lora_weights", None)
    
    if lora_files:
        pipe = load_lora_weights(pipe, lora_files, lora_weights, config)
    else:
        print("No LoRA files specified. Generating with base model.")
    
    # ç”»åƒç”Ÿæˆ
    generate_images(pipe, config, save_folder)


if __name__ == "__main__":
    main()
